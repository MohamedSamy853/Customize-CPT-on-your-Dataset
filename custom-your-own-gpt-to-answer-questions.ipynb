{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T08:17:14.590345Z","iopub.status.busy":"2023-05-01T08:17:14.589332Z","iopub.status.idle":"2023-05-01T08:18:17.599401Z","shell.execute_reply":"2023-05-01T08:18:17.598165Z","shell.execute_reply.started":"2023-05-01T08:17:14.590288Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.27.4)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.3)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.9.0)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.2)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.2)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (23.0)\n","Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.4)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.11.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.14)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.4)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (1.13.0)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch) (4.4.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mCollecting PyPDF2\n","  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing_extensions>=3.10.0.0 in /opt/conda/lib/python3.7/site-packages (from PyPDF2) (4.4.0)\n","Installing collected packages: PyPDF2\n","Successfully installed PyPDF2-3.0.1\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mCollecting python-docx\n","  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: lxml>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from python-docx) (4.9.2)\n","Building wheels for collected packages: python-docx\n","  Building wheel for python-docx (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184505 sha256=a485f666386039281c1113631b5efc59bc3141fa9616c7c2ad4ef7da5d7f8887\n","  Stored in directory: /root/.cache/pip/wheels/8d/43/ab/6dfe2e7103b24fb1148e95b265fd71d23d29bcbaa60a4a7ed9\n","Successfully built python-docx\n","Installing collected packages: python-docx\n","Successfully installed python-docx-0.8.11\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mCollecting pdfkit\n","  Downloading pdfkit-1.0.0-py3-none-any.whl (12 kB)\n","Installing collected packages: pdfkit\n","Successfully installed pdfkit-1.0.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mCollecting textacy\n","  Downloading textacy-0.11.0-py3-none-any.whl (200 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.4/200.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cytoolz>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from textacy) (0.12.0)\n","Requirement already satisfied: scikit-learn>=0.19.0 in /opt/conda/lib/python3.7/site-packages (from textacy) (1.0.2)\n","Requirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.7/site-packages (from textacy) (1.21.6)\n","Requirement already satisfied: cachetools>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from textacy) (4.2.4)\n","Requirement already satisfied: joblib>=0.13.0 in /opt/conda/lib/python3.7/site-packages (from textacy) (1.2.0)\n","Collecting jellyfish>=0.8.0\n","  Downloading jellyfish-0.11.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.7/site-packages (from textacy) (2.6.3)\n","Requirement already satisfied: requests>=2.10.0 in /opt/conda/lib/python3.7/site-packages (from textacy) (2.28.2)\n","Requirement already satisfied: spacy>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from textacy) (3.5.1)\n","Collecting pyphen>=0.10.0\n","  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /opt/conda/lib/python3.7/site-packages (from textacy) (1.7.3)\n","Requirement already satisfied: tqdm>=4.19.6 in /opt/conda/lib/python3.7/site-packages (from textacy) (4.64.1)\n","Requirement already satisfied: toolz>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from cytoolz>=0.10.1->textacy) (0.11.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.10.0->textacy) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.10.0->textacy) (1.26.14)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.10.0->textacy) (3.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.10.0->textacy) (2.1.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.19.0->textacy) (3.1.0)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (1.1.1)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (23.0)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (2.0.8)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (1.0.9)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (3.3.0)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (2.4.6)\n","Requirement already satisfied: typing-extensions<4.5.0,>=3.7.4.1 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (4.4.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (1.10.4)\n","Requirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (0.10.1)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (0.7.0)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (1.0.4)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (2.0.7)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (3.0.8)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (3.1.2)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (6.3.0)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (59.8.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (3.0.12)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->textacy) (8.1.9)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy>=3.0.0->textacy) (3.11.0)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.0.0->textacy) (0.7.9)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.0.0->textacy) (0.0.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from typer<0.8.0,>=0.3.0->spacy>=3.0.0->textacy) (8.1.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy>=3.0.0->textacy) (2.1.1)\n","Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.8.0,>=0.3.0->spacy>=3.0.0->textacy) (4.11.4)\n","Installing collected packages: pyphen, jellyfish, textacy\n","Successfully installed jellyfish-0.11.2 pyphen-0.14.0 textacy-0.11.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install transformers\n","!pip install torch\n","!pip install -U PyPDF2\n","!pip install python-docx\n","!pip install pdfkit\n","!pip install textacy"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T08:18:28.973124Z","iopub.status.busy":"2023-05-01T08:18:28.972191Z","iopub.status.idle":"2023-05-01T08:18:36.956693Z","shell.execute_reply":"2023-05-01T08:18:36.955454Z","shell.execute_reply.started":"2023-05-01T08:18:28.973075Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[32m[I 08:18:30.793 NotebookApp]\u001b[m Skipped non-installed server(s): bash-language-server, dockerfile-language-server-nodejs, javascript-typescript-langserver, jedi-language-server, julia-language-server, pyright, python-language-server, r-languageserver, sql-language-server, texlab, typescript-language-server, unified-language-server, vscode-css-languageserver-bin, vscode-html-languageserver-bin, vscode-json-languageserver-bin, yaml-language-server\n","\u001b[35m[C 08:18:32.387 NotebookApp]\u001b[m Running as root is not recommended. Use --allow-root to bypass.\n","\u001b[32m[I 08:18:34.489 NotebookApp]\u001b[m Skipped non-installed server(s): bash-language-server, dockerfile-language-server-nodejs, javascript-typescript-langserver, jedi-language-server, julia-language-server, pyright, python-language-server, r-languageserver, sql-language-server, texlab, typescript-language-server, unified-language-server, vscode-css-languageserver-bin, vscode-html-languageserver-bin, vscode-json-languageserver-bin, yaml-language-server\n","\u001b[35m[C 08:18:36.558 NotebookApp]\u001b[m Running as root is not recommended. Use --allow-root to bypass.\n"]}],"source":["!jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10 \n","!jupyter notebook --NotebookApp.rate_limit_window=3 "]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T08:24:34.855115Z","iopub.status.busy":"2023-05-01T08:24:34.854139Z","iopub.status.idle":"2023-05-01T08:24:34.861051Z","shell.execute_reply":"2023-05-01T08:24:34.859939Z","shell.execute_reply.started":"2023-05-01T08:24:34.855073Z"},"trusted":true},"outputs":[],"source":["import os\n","from PyPDF2 import PdfReader\n","import docx\n","import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling\n","from transformers import Trainer, TrainingArguments\n","import pdfkit  \n","import re\n","import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pdf_config_file = r\"C:\\Program Files\\wkhtmltopdf\\bin\\wkhtmltopdf.exe\"\n","config = pdfkit.configuration(wkhtmltopdf = pdf_config_file)  "]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["article_links =[r\"https://en.wikipedia.org/wiki/Artificial_intelligence\", \n","                r\"https://www.brookings.edu/research/how-artificial-intelligence-is-transforming-the-world\",\n","                r\"https://www.zdnet.com/article/what-is-ai-heres-everything-you-need-to-know-about-artificial-intelligence/\",\n","                r\"https://www.pewresearch.org/internet/2018/12/10/artificial-intelligence-and-the-future-of-humans/\",\n","                r\"https://spectrum.ieee.org/topic/artificial-intelligence/\",\n","                r\"https://spectrum.ieee.org/topic/artificial-intelligence/\",\n","                r\"https://www.nytimes.com/article/ai-artificial-intelligence-chatbot.html\",\n","                r\"https://www.britannica.com/technology/artificial-intelligence\",\n","                r\"https://builtin.com/artificial-intelligence\",\n","                r\"https://www.ibm.com/topics/artificial-intelligence\"]"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T08:43:22.271824Z","iopub.status.busy":"2023-05-01T08:43:22.270822Z","iopub.status.idle":"2023-05-01T08:43:22.277386Z","shell.execute_reply":"2023-05-01T08:43:22.276186Z","shell.execute_reply.started":"2023-05-01T08:43:22.271786Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/working\n"]}],"source":["print(os.getcwd())"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["def download_article(articles):\n","    if len(articles)>0:\n","        for link in tqdm.tqdm(articles):\n","            file_name = re.findall(r\"https://\\w+.(\\w+).\\w+\",link)[0]+\".pdf\"\n","            out_file = os.path.join(\"dataset\", file_name)\n","            pdfkit.from_url(link, out_file , configuration=config)\n","    else :\n","        print(\"not found links\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T18:32:28.747104Z","iopub.status.idle":"2023-04-30T18:32:28.747645Z","shell.execute_reply":"2023-04-30T18:32:28.747371Z","shell.execute_reply.started":"2023-04-30T18:32:28.747345Z"},"trusted":true},"outputs":[],"source":["from contextlib import contextmanager\n","import time\n","@contextmanager\n","def Timer():\n","    start = time.time()\n","    yield None\n","    end = time.time()\n","    print(f\"Time used is {end-start:0.3f} secounds\")"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["start Downloading...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10/10 [00:41<00:00,  4.17s/it]"]},{"name":"stdout","output_type":"stream","text":["Time used is 41.775 secounds\n","\n","Downloading is finished!\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["print(\"start Downloading...\")\n","with Timer():\n","    download_article(article_links)\n","print(\"Downloading is finished!\")"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T08:43:29.183549Z","iopub.status.busy":"2023-05-01T08:43:29.182830Z","iopub.status.idle":"2023-05-01T08:43:29.193997Z","shell.execute_reply":"2023-05-01T08:43:29.192914Z","shell.execute_reply.started":"2023-05-01T08:43:29.183511Z"},"trusted":true},"outputs":[],"source":["# Functions to read different file types\n","def read_pdf(file_path):\n","    with open(file_path, \"rb\") as file:\n","        pdf_reader = PdfReader(file)\n","        text = \"\"\n","        for page_num in range(len(pdf_reader.pages)):\n","            text += pdf_reader.pages[page_num].extract_text()\n","    return text\n","\n","def read_word(file_path):\n","    doc = docx.Document(file_path)\n","    text = \"\"\n","    for paragraph in doc.paragraphs:\n","        text += paragraph.text + \"\\n\"\n","    return text\n","\n","def read_txt(file_path):\n","    with open(file_path, \"r\") as file:\n","        text = file.read()\n","    return text\n","\n","def read_documents_from_directory(directory):\n","    combined_text = \"\"\n","    for filename in os.listdir(directory):\n","        file_path = os.path.join(directory, filename)\n","        if filename.endswith(\".pdf\"):\n","            combined_text += read_pdf(file_path)\n","        elif filename.endswith(\".docx\"):\n","            combined_text += read_word(file_path)\n","        elif filename.endswith(\".txt\"):\n","            combined_text += read_txt(file_path)\n","    return combined_text\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T08:43:57.408174Z","iopub.status.busy":"2023-05-01T08:43:57.407489Z","iopub.status.idle":"2023-05-01T08:44:12.126795Z","shell.execute_reply":"2023-05-01T08:44:12.125570Z","shell.execute_reply.started":"2023-05-01T08:43:57.408115Z"},"trusted":true},"outputs":[],"source":["def clean_text(text):\n","  \"\"\"Cleans text from noise such as any number or special character except ? , ., !, , and also remove specs and taps and any noise.\n","\n","  Args:\n","    text: The text to be cleaned.\n","\n","  Returns:\n","    The cleaned text.\n","  \"\"\"\n","\n","  # Remove any numbers from the text.\n","  text = re.sub(r'\\d+', '', text)\n","\n","  # Remove any special characters from the text except ? , ., !, ,.\n","  text = re.sub(r'[^\\w\\s?.!,]', '', text)\n","\n","  # Remove any spaces from the beginning and end of the text.\n","  text = text.strip()\n","\n","  # Remove any HTML tags from the text.\n","  text = re.sub(r'<[^>]+>', '', text)\n","\n","  # Remove any links from the text.\n","  text = re.sub(r'https?://[^ ]+', '', text)\n","\n","  # Remove any punctuation marks from the text.\n","  text = re.sub(r'[,!?.:;]', '', text)\n","   #remove bracket\n","  text = re.sub(r\"\\[[^\\[\\]]*\\]\", \" \", text)\n","  text = re.sub(r\"(?:^|\\s)[&#<>{}\\[\\]+|\\\\:-]{1,}(?:\\s|$)\" , \" \", text)\n","  text = re.sub(r\"(?:^|\\s)[\\-=\\+]{2,}(?:\\s|$)\",\" \", text)\n","  text = re.sub(r\"\\s+\" ,\" \", text)\n","\n","  return text\n","\n","#character normalization \n","import textacy.preprocessing as tprep\n","def normalize(text):\n","    text = tprep.normalize.hyphenated_words(text)\n","    text = tprep.normalize.quotation_marks(text)\n","    text = tprep.normalize.unicode(text)\n","    text = tprep.remove.accents(text)\n","    return text \n","def more_clean(text):\n","    text = tprep.replace.hashtags(text,\" \")\n","    text = tprep.replace.emojis(text , \" \")\n","    text = tprep.remove.html_tags(text)\n","    text = tprep.replace.urls(text)\n","    return text    \n","\n","def clean(text):\n","  text = clean_text(text)\n","  text = normalize(text)\n","  text = more_clean(text)\n","  return text\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T08:44:36.475556Z","iopub.status.busy":"2023-05-01T08:44:36.474521Z","iopub.status.idle":"2023-05-01T08:44:36.486538Z","shell.execute_reply":"2023-05-01T08:44:36.485328Z","shell.execute_reply.started":"2023-05-01T08:44:36.475505Z"},"trusted":true},"outputs":[],"source":["def train_chatbot(directory, model_output_path, train_fraction=0.8):\n","    # Read documents from the directory\n","    combined_text = read_documents_from_directory(directory)\n","    combined_text = re.sub(r'\\n+', '\\n', combined_text).strip()  # Remove excess newline characters\n","\n","    # Split the text into training and validation sets\n","    split_index = int(train_fraction * len(combined_text))\n","    train_text = combined_text[:split_index]\n","    val_text = combined_text[split_index:]\n","    train_text = clean(train_text).encode(\"utf-8\").strip()\n","    val_text = clean(val_text).encode(\"utf-8\").strip()\n","    # Save the training and validation data as text files\n","    with open(\"train.txt\", \"wb\") as f:\n","        f.write(train_text)\n","    with open(\"val.txt\", \"wb\") as f:\n","        f.write(val_text)\n","    del combined_text, train_text, val_text\n","    \n","    print(\"start trainning ....\")\n","\n","    # Set up the tokenizer and model\n","    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")  #also try gpt2, gpt2-large and  gpt2-xl\n","    model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")  #also try gpt2, gpt2-large and gpt2-large, also gpt2-xl\n","\n","    # Prepare the dataset\n","    train_dataset = TextDataset(tokenizer=tokenizer, file_path=\"train.txt\", block_size=128)\n","    val_dataset = TextDataset(tokenizer=tokenizer, file_path=\"val.txt\", block_size=128)\n","    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False) #to make a padding \n","\n","    # Set up the training arguments\n","    training_args = TrainingArguments(\n","        output_dir=model_output_path,\n","        overwrite_output_dir=True,\n","        per_device_train_batch_size=4,\n","        per_device_eval_batch_size=4,\n","        num_train_epochs=100,\n","        save_steps=10_000,\n","        save_total_limit=2,\n","        logging_dir='./logs',\n","    )\n","\n","    # Train the model\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        data_collator=data_collator,\n","        train_dataset=train_dataset,\n","        eval_dataset=val_dataset,\n","    )\n","\n","    trainer.train()\n","    trainer.save_model(model_output_path)\n","    \n","    # Save the tokenizer\n","    tokenizer.save_pretrained(model_output_path)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T08:45:05.267109Z","iopub.status.busy":"2023-05-01T08:45:05.266716Z","iopub.status.idle":"2023-05-01T08:45:05.273806Z","shell.execute_reply":"2023-05-01T08:45:05.272574Z","shell.execute_reply.started":"2023-05-01T08:45:05.267074Z"},"trusted":true},"outputs":[],"source":["def generate_response(model, tokenizer, prompt, max_length=100):\n","    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n","    \n","    # Create the attention mask and pad token id\n","    attention_mask = torch.ones_like(input_ids)\n","    pad_token_id = tokenizer.eos_token_id\n","\n","    output = model.generate(\n","        input_ids,\n","        max_length=max_length,\n","        num_return_sequences=1,\n","        attention_mask=attention_mask,\n","        pad_token_id=pad_token_id\n","    )\n","\n","    return tokenizer.decode(output[0], skip_special_tokens=True)\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T08:45:07.394520Z","iopub.status.busy":"2023-05-01T08:45:07.393775Z","iopub.status.idle":"2023-05-01T08:45:07.408638Z","shell.execute_reply":"2023-05-01T08:45:07.407545Z","shell.execute_reply.started":"2023-05-01T08:45:07.394480Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['ARTIFICIAL INTELLIGENCEr.pdf',\n"," 'EPRS_STU(2020)641547_EN.pdf',\n"," 'com.pdf',\n"," 'wikipedia.pdf',\n"," 'zdnet.pdf',\n"," 'ArtificialIntelligenceDefinitionEthicsandStandards.docx',\n"," 'britannica.pdf',\n"," 'artificial_intelligence_tutorial.pdf']"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["os.listdir(\"/kaggle/input/dataset\")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T08:45:12.823940Z","iopub.status.busy":"2023-05-01T08:45:12.822879Z","iopub.status.idle":"2023-05-01T08:45:12.828121Z","shell.execute_reply":"2023-05-01T08:45:12.826925Z","shell.execute_reply.started":"2023-05-01T08:45:12.823880Z"},"trusted":true},"outputs":[],"source":["os.mkdir(\"model_output\")"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T08:45:16.248041Z","iopub.status.busy":"2023-05-01T08:45:16.246940Z","iopub.status.idle":"2023-05-01T08:45:16.255643Z","shell.execute_reply":"2023-05-01T08:45:16.254542Z","shell.execute_reply.started":"2023-05-01T08:45:16.247998Z"},"trusted":true},"outputs":[],"source":["\n","def main():\n","    directory = \"/kaggle/input/dataset\"  # Replace with the path to your directory containing the files\n","    model_output_path = \"./model_output/\"\n","\n","    # Train the chatbot\n","    train_chatbot(directory, model_output_path)\n","\n","    # Load the fine-tuned model and tokenizer\n","    model = GPT2LMHeadModel.from_pretrained(model_output_path)\n","    tokenizer = GPT2Tokenizer.from_pretrained(model_output_path)\n","\n","    # Test the chatbot\n","    prompt = \"what is Artificial intelligence?\"  # Replace with your desired prompt\n","    response = generate_response(model, tokenizer, prompt)\n","    print(\"Generated response:\", response)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T08:45:21.312242Z","iopub.status.busy":"2023-05-01T08:45:21.311850Z","iopub.status.idle":"2023-05-01T10:01:11.765666Z","shell.execute_reply":"2023-05-01T10:01:11.764602Z","shell.execute_reply.started":"2023-05-01T08:45:21.312209Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["start trainning ....\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eb9d0b695a98422b8c672ec55b75dbbe","version_major":2,"version_minor":0},"text/plain":["Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1f349cc1d5b2444bb466a3c509335e6a","version_major":2,"version_minor":0},"text/plain":["Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e72c7f3ac11d4adea82a1f3d57b796d9","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9a4cd50cf8124612905fc8b6055fdabd","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c94c606ca7274371a88d918745d5534b","version_major":2,"version_minor":0},"text/plain":["Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/transformers/data/datasets/language_modeling.py:57: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["wandb version 0.15.0 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.14.0"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20230501_084853-7bh8psl4</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/starks2/huggingface/runs/7bh8psl4' target=\"_blank\">light-butterfly-5</a></strong> to <a href='https://wandb.ai/starks2/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/starks2/huggingface' target=\"_blank\">https://wandb.ai/starks2/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/starks2/huggingface/runs/7bh8psl4' target=\"_blank\">https://wandb.ai/starks2/huggingface/runs/7bh8psl4</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='14900' max='14900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [14900/14900 1:11:23, Epoch 100/100]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>3.395900</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.786000</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.711700</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.259600</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.134600</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.085800</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.061600</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.048900</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.038700</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.033400</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>0.027600</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>0.024100</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>0.021800</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>0.020300</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>0.018600</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>0.017100</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>0.016100</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>0.014900</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>0.014700</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>0.013200</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>0.013200</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>0.011800</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>0.011500</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>0.011100</td>\n","    </tr>\n","    <tr>\n","      <td>12500</td>\n","      <td>0.010600</td>\n","    </tr>\n","    <tr>\n","      <td>13000</td>\n","      <td>0.010300</td>\n","    </tr>\n","    <tr>\n","      <td>13500</td>\n","      <td>0.009700</td>\n","    </tr>\n","    <tr>\n","      <td>14000</td>\n","      <td>0.009500</td>\n","    </tr>\n","    <tr>\n","      <td>14500</td>\n","      <td>0.009000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Generated response: what is Artificial intelligence? What is machine learning Image Bing Image CreatorZDNET The biggest quality that sets AI aside from other computer science topics is the ability to easily automate tasks by employing machine learning which lets computers learn from different experiences rather than being explicitly programmed to perform each task This capability is what many refer to as AI but machine learning is actually a subset of artificial intelligence Machine learning involves a system being trained on large amounts of data so it can learn from mistakes and recognize patterns in order to accurately make\n"]}],"source":["if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T10:30:16.323563Z","iopub.status.busy":"2023-05-01T10:30:16.323125Z","iopub.status.idle":"2023-05-01T10:30:16.334567Z","shell.execute_reply":"2023-05-01T10:30:16.332039Z","shell.execute_reply.started":"2023-05-01T10:30:16.323524Z"},"trusted":true},"outputs":[],"source":["import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T10:30:19.485551Z","iopub.status.busy":"2023-05-01T10:30:19.484812Z","iopub.status.idle":"2023-05-01T10:30:19.494020Z","shell.execute_reply":"2023-05-01T10:30:19.492510Z","shell.execute_reply.started":"2023-05-01T10:30:19.485512Z"},"trusted":true},"outputs":[],"source":["def generate_response(model, tokenizer, prompt, max_length=250):\n","    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n","    \n","    # Create the attention mask and pad token id\n","    attention_mask = torch.ones_like(input_ids)\n","    pad_token_id = tokenizer.eos_token_id\n","\n","    output = model.generate(\n","        input_ids,\n","        max_length=max_length,\n","        num_return_sequences=1,\n","        attention_mask=attention_mask,\n","        pad_token_id=pad_token_id\n","    )\n","\n","    return tokenizer.decode(output[0], skip_special_tokens=True)\n"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T10:30:22.502340Z","iopub.status.busy":"2023-05-01T10:30:22.501368Z","iopub.status.idle":"2023-05-01T10:30:26.540881Z","shell.execute_reply":"2023-05-01T10:30:26.539610Z","shell.execute_reply.started":"2023-05-01T10:30:22.502283Z"},"trusted":true},"outputs":[],"source":["model_path = \"/kaggle/working/model_output\"\n","# Load the fine-tuned model and tokenizer\n","my_chat_model = GPT2LMHeadModel.from_pretrained(model_path)\n","my_chat_tokenizer = GPT2Tokenizer.from_pretrained(model_path)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T10:30:38.389606Z","iopub.status.busy":"2023-05-01T10:30:38.388981Z","iopub.status.idle":"2023-05-01T10:30:50.783661Z","shell.execute_reply":"2023-05-01T10:30:50.780781Z","shell.execute_reply.started":"2023-05-01T10:30:38.389565Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Generated response: summarize what you learned about AI? What are some recent examples of AI that you are most excited about and are looking forward to learning about next What are some of the most anticipated technologies in the next decade and what are some of the most anticipated technologies in the next decade What is machine learning and how does it work why does it matter and what can we do about it What is deep learning and how does it work why does it matter and what can we do about it What is fuzzy logic and\n"]}],"source":["prompt = \"summarize what you learned about AI?\"  # Replace with your desired prompt\n","#prompt = \"What is the most promising future technology?\"\n","response = generate_response(my_chat_model, my_chat_tokenizer, prompt, max_length=100)  #\n","print(\"Generated response:\", response)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T10:31:28.575124Z","iopub.status.busy":"2023-05-01T10:31:28.574348Z","iopub.status.idle":"2023-05-01T10:31:40.592994Z","shell.execute_reply":"2023-05-01T10:31:40.591715Z","shell.execute_reply.started":"2023-05-01T10:31:28.575081Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Generated response: what impact of artificial intelligience in market or socity or industry? What can we do about it Expert Group on Liability and New Technologies New Technologies Formation Liability for artificial intelligence and other emerging digital technologies European Commission High Level Expert Group on Arti ficial Intelligence Ethics guidelines for trustworthy AI European Commission High Level Expert Group on Artificial Intelligence Trustworthy AI assessment list European Commission High Level Expert Group on Artificial Intelligence Policy and investment recommendations for trustworthy AI European Commission Mazzucato M Mission oriented research i n\n"]}],"source":["prompt = \"what impact of artificial intelligience in market or socity or industry?\"  # Replace with your desired prompt\n","#prompt = \"What is the most promising future technology?\"\n","response = generate_response(my_chat_model, my_chat_tokenizer, prompt, max_length=100)  #\n","print(\"Generated response:\", response)"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T10:34:17.495693Z","iopub.status.busy":"2023-05-01T10:34:17.494929Z","iopub.status.idle":"2023-05-01T10:34:17.506326Z","shell.execute_reply":"2023-05-01T10:34:17.504142Z","shell.execute_reply.started":"2023-05-01T10:34:17.495648Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'/kaggle/working'"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["os.getcwd()"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T10:34:47.988655Z","iopub.status.busy":"2023-05-01T10:34:47.988038Z","iopub.status.idle":"2023-05-01T10:40:14.973962Z","shell.execute_reply":"2023-05-01T10:40:14.972431Z","shell.execute_reply.started":"2023-05-01T10:34:47.988615Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'/kaggle/working/model_files.zip'"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["import shutil\n","shutil.make_archive(\"model_files\", 'zip', \"model_output\")"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T11:23:24.525510Z","iopub.status.busy":"2023-05-01T11:23:24.524493Z","iopub.status.idle":"2023-05-01T11:23:24.536021Z","shell.execute_reply":"2023-05-01T11:23:24.534936Z","shell.execute_reply.started":"2023-05-01T11:23:24.525458Z"},"trusted":true},"outputs":[{"data":{"text/html":["<a href='model_files.zip' target='_blank'>model_files.zip</a><br>"],"text/plain":["/kaggle/working/model_files.zip"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["from IPython.display import FileLink\n","FileLink(r'model_files.zip')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"57bc2b6ce032b5f0e93daa91901b7ea38a856826ef43aa9e95b6d3999f5310df"}}},"nbformat":4,"nbformat_minor":4}
